Trait -> Smiling
==> Options: Namespace(att_file='/scratch/saraansh/Attribute-and-Simile-Classifiers-for-Face-Verification/attr.pkl', batch_size=128, gamma=0.1, lr=0.001, model='mobilenet', num_epochs=50, sample_thresh=0.75, save_path='/scratch/mobilenet_neural_net_att_Smiling', step_size=12, test_dir='/scratch/saraansh/Attribute-and-Simile-Classifiers-for-Face-Verification/test', testbatch_size=128, train_dir='/scratch/saraansh/Attribute-and-Simile-Classifiers-for-Face-Verification/train', trait='Smiling', weight_decay=4e-05, workers=10)
Using GPU: True
train /scratch/saraansh/Attribute-and-Simile-Classifiers-for-Face-Verification/train 9902
train Positive ->  4244
train Negative ->  5658
test /scratch/saraansh/Attribute-and-Simile-Classifiers-for-Face-Verification/test 2830
test Positive ->  1188
test Negative ->  1642
MobileNetV2(
  (features): Sequential(
    (0): ConvBNReLU(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): ConvBNReLU(
          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (1): ConvBNReLU(
          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (18): ConvBNReLU(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
  )
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=2, bias=True)
  )
)
Epoch 0/49
----------
==================================================================================
Train Loss:  tensor(0.3028, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 1/49
----------
==================================================================================
Train Loss:  tensor(0.1824, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 2/49
----------
==================================================================================
Train Loss:  tensor(0.1383, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 3/49
----------
==================================================================================
Train Loss:  tensor(0.0996, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 4/49
----------
==================================================================================
Train Loss:  tensor(0.0776, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 5/49
----------
==================================================================================
Train Loss:  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 6/49
----------
==================================================================================
Train Loss:  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 7/49
----------
==================================================================================
Train Loss:  tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 8/49
----------
==================================================================================
Train Loss:  tensor(0.0336, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 9/49
----------
==================================================================================
Train Loss:  tensor(0.0360, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 10/49
----------
==================================================================================
Train Loss:  tensor(0.0280, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 11/49
----------
==================================================================================
Train Loss:  tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 12/49
----------
==================================================================================
Train Loss:  tensor(0.0129, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 13/49
----------
==================================================================================
Train Loss:  tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 14/49
----------
==================================================================================
Train Loss:  tensor(0.0020, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 15/49
----------
==================================================================================
Train Loss:  tensor(0.0008, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 16/49
----------
==================================================================================
Train Loss:  tensor(0.0006, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 17/49
----------
==================================================================================
Train Loss:  tensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 18/49
----------
==================================================================================
Train Loss:  tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 19/49
----------
==================================================================================
Train Loss:  tensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 20/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 21/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 22/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 23/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 24/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 25/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 26/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 27/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 28/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 29/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 30/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 31/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 32/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 33/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 34/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 35/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 36/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 37/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 38/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 39/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 40/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 41/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 42/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 43/49
----------
==================================================================================
Train Loss:  tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 44/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 45/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 46/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 47/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 48/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
Epoch 49/49
----------
==================================================================================
Train Loss:  tensor(0.0001, device='cuda:0', grad_fn=<DivBackward0>)
==> Saving models ...
Creating Directory:  /scratch/mobilenet_neural_net_att_Smiling/models
Confusion Matrix -> 
[[1487  155]
 [ 115 1073]]
              precision    recall  f1-score   support

           0       0.93      0.91      0.92      1642
           1       0.87      0.90      0.89      1188

    accuracy                           0.90      2830
   macro avg       0.90      0.90      0.90      2830
weighted avg       0.91      0.90      0.90      2830

